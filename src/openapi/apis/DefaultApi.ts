/* tslint:disable */
/* eslint-disable */
/**
 * VOICEVOX ENGINE
 * VOICEVOXの音声合成エンジンです。
 *
 * The version of the OpenAPI document: 0.10.preview.2
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import * as runtime from '../runtime';
import {
    AccentPhrase,
    AccentPhraseFromJSON,
    AccentPhraseToJSON,
    AudioQuery,
    AudioQueryFromJSON,
    AudioQueryToJSON,
    HTTPValidationError,
    HTTPValidationErrorFromJSON,
    HTTPValidationErrorToJSON,
    ParseKanaBadRequest,
    ParseKanaBadRequestFromJSON,
    ParseKanaBadRequestToJSON,
    Preset,
    PresetFromJSON,
    PresetToJSON,
    Speaker,
    SpeakerFromJSON,
    SpeakerToJSON,
    SpeakerInfo,
    SpeakerInfoFromJSON,
    SpeakerInfoToJSON,
} from '../models';

export interface AccentPhrasesAccentPhrasesPostRequest {
    text: string;
    speaker: number;
    isKana?: boolean;
    enableInterrogative?: boolean;
}

export interface AudioQueryAudioQueryPostRequest {
    text: string;
    speaker: number;
    enableInterrogative?: boolean;
}

export interface AudioQueryFromPresetAudioQueryFromPresetPostRequest {
    text: string;
    presetId: number;
    enableInterrogative?: boolean;
}

export interface CancellableSynthesisCancellableSynthesisPostRequest {
    speaker: number;
    audioQuery: AudioQuery;
}

export interface ConnectWavesConnectWavesPostRequest {
    requestBody: Array<string>;
}

export interface MoraDataMoraDataPostRequest {
    speaker: number;
    accentPhrase: Array<AccentPhrase>;
}

export interface MoraLengthMoraLengthPostRequest {
    speaker: number;
    accentPhrase: Array<AccentPhrase>;
}

export interface MoraPitchMoraPitchPostRequest {
    speaker: number;
    accentPhrase: Array<AccentPhrase>;
}

export interface MultiSynthesisMultiSynthesisPostRequest {
    speaker: number;
    audioQuery: Array<AudioQuery>;
}

export interface SpeakerInfoSpeakerInfoGetRequest {
    speakerUuid: string;
}

export interface SynthesisMorphingSynthesisMorphingPostRequest {
    baseSpeaker: number;
    targetSpeaker: number;
    morphRate: number;
    audioQuery: AudioQuery;
}

export interface SynthesisSynthesisPostRequest {
    speaker: number;
    audioQuery: AudioQuery;
}

/**
 * DefaultApi - interface
 * 
 * @export
 * @interface DefaultApiInterface
 */
export interface DefaultApiInterface {
    /**
     * テキストからアクセント句を得ます。 is_kanaが`true`のとき、テキストは次のようなAquesTalkライクな記法に従う読み仮名として処理されます。デフォルトは`false`です。 * 全てのカナはカタカナで記述される * アクセント句は`/`または`、`で区切る。`、`で区切った場合に限り無音区間が挿入される。 * カナの手前に`_`を入れるとそのカナは無声化される * アクセント位置を`\'`で指定する。全てのアクセント句にはアクセント位置を1つ指定する必要がある。
     * @summary テキストからアクセント句を得る
     * @param {string} text 
     * @param {number} speaker 
     * @param {boolean} [isKana] 
     * @param {boolean} [enableInterrogative] 疑問系のテキストが与えられたら自動調整する機能を有効にする。現在は長音を付け足すことで擬似的に実装される
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    accentPhrasesAccentPhrasesPostRaw(requestParameters: AccentPhrasesAccentPhrasesPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>>;

    /**
     * テキストからアクセント句を得ます。 is_kanaが`true`のとき、テキストは次のようなAquesTalkライクな記法に従う読み仮名として処理されます。デフォルトは`false`です。 * 全てのカナはカタカナで記述される * アクセント句は`/`または`、`で区切る。`、`で区切った場合に限り無音区間が挿入される。 * カナの手前に`_`を入れるとそのカナは無声化される * アクセント位置を`\'`で指定する。全てのアクセント句にはアクセント位置を1つ指定する必要がある。
     * テキストからアクセント句を得る
     */
    accentPhrasesAccentPhrasesPost(requestParameters: AccentPhrasesAccentPhrasesPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>>;

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * @summary 音声合成用のクエリを作成する
     * @param {string} text 
     * @param {number} speaker 
     * @param {boolean} [enableInterrogative] 疑問系のテキストが与えられたら自動調整する機能を有効にする。現在は長音を付け足すことで擬似的に実装される
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    audioQueryAudioQueryPostRaw(requestParameters: AudioQueryAudioQueryPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<AudioQuery>>;

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリを作成する
     */
    audioQueryAudioQueryPost(requestParameters: AudioQueryAudioQueryPostRequest, initOverrides?: RequestInit): Promise<AudioQuery>;

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * @summary 音声合成用のクエリをプリセットを用いて作成する
     * @param {string} text 
     * @param {number} presetId 
     * @param {boolean} [enableInterrogative] 疑問系のテキストが与えられたら自動調整する機能を有効にする。現在は長音を付け足すことで擬似的に実装される
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    audioQueryFromPresetAudioQueryFromPresetPostRaw(requestParameters: AudioQueryFromPresetAudioQueryFromPresetPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<AudioQuery>>;

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリをプリセットを用いて作成する
     */
    audioQueryFromPresetAudioQueryFromPresetPost(requestParameters: AudioQueryFromPresetAudioQueryFromPresetPostRequest, initOverrides?: RequestInit): Promise<AudioQuery>;

    /**
     * 
     * @summary 音声合成する（キャンセル可能）
     * @param {number} speaker 
     * @param {AudioQuery} audioQuery 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    cancellableSynthesisCancellableSynthesisPostRaw(requestParameters: CancellableSynthesisCancellableSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>>;

    /**
     * 音声合成する（キャンセル可能）
     */
    cancellableSynthesisCancellableSynthesisPost(requestParameters: CancellableSynthesisCancellableSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob>;

    /**
     * base64エンコードされたwavデータを一纏めにし、wavファイルで返します。
     * @summary base64エンコードされた複数のwavデータを一つに結合する
     * @param {Array<string>} requestBody 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    connectWavesConnectWavesPostRaw(requestParameters: ConnectWavesConnectWavesPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>>;

    /**
     * base64エンコードされたwavデータを一纏めにし、wavファイルで返します。
     * base64エンコードされた複数のwavデータを一つに結合する
     */
    connectWavesConnectWavesPost(requestParameters: ConnectWavesConnectWavesPostRequest, initOverrides?: RequestInit): Promise<Blob>;

    /**
     * エンジンが保持しているプリセットの設定を返します  Returns ------- presets: List[Preset]     プリセットのリスト
     * @summary Get Presets
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    getPresetsPresetsGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<Preset>>>;

    /**
     * エンジンが保持しているプリセットの設定を返します  Returns ------- presets: List[Preset]     プリセットのリスト
     * Get Presets
     */
    getPresetsPresetsGet(initOverrides?: RequestInit): Promise<Array<Preset>>;

    /**
     * 
     * @summary アクセント句から音高・音素長を得る
     * @param {number} speaker 
     * @param {Array<AccentPhrase>} accentPhrase 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    moraDataMoraDataPostRaw(requestParameters: MoraDataMoraDataPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>>;

    /**
     * アクセント句から音高・音素長を得る
     */
    moraDataMoraDataPost(requestParameters: MoraDataMoraDataPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>>;

    /**
     * 
     * @summary アクセント句から音素長を得る
     * @param {number} speaker 
     * @param {Array<AccentPhrase>} accentPhrase 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    moraLengthMoraLengthPostRaw(requestParameters: MoraLengthMoraLengthPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>>;

    /**
     * アクセント句から音素長を得る
     */
    moraLengthMoraLengthPost(requestParameters: MoraLengthMoraLengthPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>>;

    /**
     * 
     * @summary アクセント句から音高を得る
     * @param {number} speaker 
     * @param {Array<AccentPhrase>} accentPhrase 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    moraPitchMoraPitchPostRaw(requestParameters: MoraPitchMoraPitchPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>>;

    /**
     * アクセント句から音高を得る
     */
    moraPitchMoraPitchPost(requestParameters: MoraPitchMoraPitchPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>>;

    /**
     * 
     * @summary 複数まとめて音声合成する
     * @param {number} speaker 
     * @param {Array<AudioQuery>} audioQuery 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    multiSynthesisMultiSynthesisPostRaw(requestParameters: MultiSynthesisMultiSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>>;

    /**
     * 複数まとめて音声合成する
     */
    multiSynthesisMultiSynthesisPost(requestParameters: MultiSynthesisMultiSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob>;

    /**
     * 指定されたspeaker_uuidに関する情報をjson形式で返します。 画像や音声はbase64エンコードされたものが返されます。  Returns ------- ret_data: SpeakerInfo
     * @summary Speaker Info
     * @param {string} speakerUuid 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    speakerInfoSpeakerInfoGetRaw(requestParameters: SpeakerInfoSpeakerInfoGetRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<SpeakerInfo>>;

    /**
     * 指定されたspeaker_uuidに関する情報をjson形式で返します。 画像や音声はbase64エンコードされたものが返されます。  Returns ------- ret_data: SpeakerInfo
     * Speaker Info
     */
    speakerInfoSpeakerInfoGet(requestParameters: SpeakerInfoSpeakerInfoGetRequest, initOverrides?: RequestInit): Promise<SpeakerInfo>;

    /**
     * 
     * @summary Speakers
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    speakersSpeakersGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<Speaker>>>;

    /**
     * Speakers
     */
    speakersSpeakersGet(initOverrides?: RequestInit): Promise<Array<Speaker>>;

    /**
     * 指定された2人の話者で音声を合成、指定した割合でモーフィングした音声を得ます。 モーフィングの割合は`morph_rate`で指定でき、0.0でベースの話者、1.0でターゲットの話者に近づきます。
     * @summary 2人の話者でモーフィングした音声を合成する
     * @param {number} baseSpeaker 
     * @param {number} targetSpeaker 
     * @param {number} morphRate 
     * @param {AudioQuery} audioQuery 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    synthesisMorphingSynthesisMorphingPostRaw(requestParameters: SynthesisMorphingSynthesisMorphingPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>>;

    /**
     * 指定された2人の話者で音声を合成、指定した割合でモーフィングした音声を得ます。 モーフィングの割合は`morph_rate`で指定でき、0.0でベースの話者、1.0でターゲットの話者に近づきます。
     * 2人の話者でモーフィングした音声を合成する
     */
    synthesisMorphingSynthesisMorphingPost(requestParameters: SynthesisMorphingSynthesisMorphingPostRequest, initOverrides?: RequestInit): Promise<Blob>;

    /**
     * 
     * @summary 音声合成する
     * @param {number} speaker 
     * @param {AudioQuery} audioQuery 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    synthesisSynthesisPostRaw(requestParameters: SynthesisSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>>;

    /**
     * 音声合成する
     */
    synthesisSynthesisPost(requestParameters: SynthesisSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob>;

    /**
     * 
     * @summary Version
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof DefaultApiInterface
     */
    versionVersionGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<any>>;

    /**
     * Version
     */
    versionVersionGet(initOverrides?: RequestInit): Promise<any>;

}

/**
 * 
 */
export class DefaultApi extends runtime.BaseAPI implements DefaultApiInterface {

    /**
     * テキストからアクセント句を得ます。 is_kanaが`true`のとき、テキストは次のようなAquesTalkライクな記法に従う読み仮名として処理されます。デフォルトは`false`です。 * 全てのカナはカタカナで記述される * アクセント句は`/`または`、`で区切る。`、`で区切った場合に限り無音区間が挿入される。 * カナの手前に`_`を入れるとそのカナは無声化される * アクセント位置を`\'`で指定する。全てのアクセント句にはアクセント位置を1つ指定する必要がある。
     * テキストからアクセント句を得る
     */
    async accentPhrasesAccentPhrasesPostRaw(requestParameters: AccentPhrasesAccentPhrasesPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>> {
        if (requestParameters.text === null || requestParameters.text === undefined) {
            throw new runtime.RequiredError('text','Required parameter requestParameters.text was null or undefined when calling accentPhrasesAccentPhrasesPost.');
        }

        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling accentPhrasesAccentPhrasesPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.text !== undefined) {
            queryParameters['text'] = requestParameters.text;
        }

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        if (requestParameters.isKana !== undefined) {
            queryParameters['is_kana'] = requestParameters.isKana;
        }

        if (requestParameters.enableInterrogative !== undefined) {
            queryParameters['enable_interrogative'] = requestParameters.enableInterrogative;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/accent_phrases`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(AccentPhraseFromJSON));
    }

    /**
     * テキストからアクセント句を得ます。 is_kanaが`true`のとき、テキストは次のようなAquesTalkライクな記法に従う読み仮名として処理されます。デフォルトは`false`です。 * 全てのカナはカタカナで記述される * アクセント句は`/`または`、`で区切る。`、`で区切った場合に限り無音区間が挿入される。 * カナの手前に`_`を入れるとそのカナは無声化される * アクセント位置を`\'`で指定する。全てのアクセント句にはアクセント位置を1つ指定する必要がある。
     * テキストからアクセント句を得る
     */
    async accentPhrasesAccentPhrasesPost(requestParameters: AccentPhrasesAccentPhrasesPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>> {
        const response = await this.accentPhrasesAccentPhrasesPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリを作成する
     */
    async audioQueryAudioQueryPostRaw(requestParameters: AudioQueryAudioQueryPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<AudioQuery>> {
        if (requestParameters.text === null || requestParameters.text === undefined) {
            throw new runtime.RequiredError('text','Required parameter requestParameters.text was null or undefined when calling audioQueryAudioQueryPost.');
        }

        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling audioQueryAudioQueryPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.text !== undefined) {
            queryParameters['text'] = requestParameters.text;
        }

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        if (requestParameters.enableInterrogative !== undefined) {
            queryParameters['enable_interrogative'] = requestParameters.enableInterrogative;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/audio_query`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => AudioQueryFromJSON(jsonValue));
    }

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリを作成する
     */
    async audioQueryAudioQueryPost(requestParameters: AudioQueryAudioQueryPostRequest, initOverrides?: RequestInit): Promise<AudioQuery> {
        const response = await this.audioQueryAudioQueryPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリをプリセットを用いて作成する
     */
    async audioQueryFromPresetAudioQueryFromPresetPostRaw(requestParameters: AudioQueryFromPresetAudioQueryFromPresetPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<AudioQuery>> {
        if (requestParameters.text === null || requestParameters.text === undefined) {
            throw new runtime.RequiredError('text','Required parameter requestParameters.text was null or undefined when calling audioQueryFromPresetAudioQueryFromPresetPost.');
        }

        if (requestParameters.presetId === null || requestParameters.presetId === undefined) {
            throw new runtime.RequiredError('presetId','Required parameter requestParameters.presetId was null or undefined when calling audioQueryFromPresetAudioQueryFromPresetPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.text !== undefined) {
            queryParameters['text'] = requestParameters.text;
        }

        if (requestParameters.presetId !== undefined) {
            queryParameters['preset_id'] = requestParameters.presetId;
        }

        if (requestParameters.enableInterrogative !== undefined) {
            queryParameters['enable_interrogative'] = requestParameters.enableInterrogative;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/audio_query_from_preset`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => AudioQueryFromJSON(jsonValue));
    }

    /**
     * クエリの初期値を得ます。ここで得られたクエリはそのまま音声合成に利用できます。各値の意味は`Schemas`を参照してください。
     * 音声合成用のクエリをプリセットを用いて作成する
     */
    async audioQueryFromPresetAudioQueryFromPresetPost(requestParameters: AudioQueryFromPresetAudioQueryFromPresetPostRequest, initOverrides?: RequestInit): Promise<AudioQuery> {
        const response = await this.audioQueryFromPresetAudioQueryFromPresetPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * 音声合成する（キャンセル可能）
     */
    async cancellableSynthesisCancellableSynthesisPostRaw(requestParameters: CancellableSynthesisCancellableSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling cancellableSynthesisCancellableSynthesisPost.');
        }

        if (requestParameters.audioQuery === null || requestParameters.audioQuery === undefined) {
            throw new runtime.RequiredError('audioQuery','Required parameter requestParameters.audioQuery was null or undefined when calling cancellableSynthesisCancellableSynthesisPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/cancellable_synthesis`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: AudioQueryToJSON(requestParameters.audioQuery),
        }, initOverrides);

        return new runtime.BlobApiResponse(response);
    }

    /**
     * 音声合成する（キャンセル可能）
     */
    async cancellableSynthesisCancellableSynthesisPost(requestParameters: CancellableSynthesisCancellableSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob> {
        const response = await this.cancellableSynthesisCancellableSynthesisPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * base64エンコードされたwavデータを一纏めにし、wavファイルで返します。
     * base64エンコードされた複数のwavデータを一つに結合する
     */
    async connectWavesConnectWavesPostRaw(requestParameters: ConnectWavesConnectWavesPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>> {
        if (requestParameters.requestBody === null || requestParameters.requestBody === undefined) {
            throw new runtime.RequiredError('requestBody','Required parameter requestParameters.requestBody was null or undefined when calling connectWavesConnectWavesPost.');
        }

        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/connect_waves`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: requestParameters.requestBody,
        }, initOverrides);

        return new runtime.BlobApiResponse(response);
    }

    /**
     * base64エンコードされたwavデータを一纏めにし、wavファイルで返します。
     * base64エンコードされた複数のwavデータを一つに結合する
     */
    async connectWavesConnectWavesPost(requestParameters: ConnectWavesConnectWavesPostRequest, initOverrides?: RequestInit): Promise<Blob> {
        const response = await this.connectWavesConnectWavesPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * エンジンが保持しているプリセットの設定を返します  Returns ------- presets: List[Preset]     プリセットのリスト
     * Get Presets
     */
    async getPresetsPresetsGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<Preset>>> {
        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/presets`,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(PresetFromJSON));
    }

    /**
     * エンジンが保持しているプリセットの設定を返します  Returns ------- presets: List[Preset]     プリセットのリスト
     * Get Presets
     */
    async getPresetsPresetsGet(initOverrides?: RequestInit): Promise<Array<Preset>> {
        const response = await this.getPresetsPresetsGetRaw(initOverrides);
        return await response.value();
    }

    /**
     * アクセント句から音高・音素長を得る
     */
    async moraDataMoraDataPostRaw(requestParameters: MoraDataMoraDataPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling moraDataMoraDataPost.');
        }

        if (requestParameters.accentPhrase === null || requestParameters.accentPhrase === undefined) {
            throw new runtime.RequiredError('accentPhrase','Required parameter requestParameters.accentPhrase was null or undefined when calling moraDataMoraDataPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/mora_data`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: requestParameters.accentPhrase.map(AccentPhraseToJSON),
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(AccentPhraseFromJSON));
    }

    /**
     * アクセント句から音高・音素長を得る
     */
    async moraDataMoraDataPost(requestParameters: MoraDataMoraDataPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>> {
        const response = await this.moraDataMoraDataPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * アクセント句から音素長を得る
     */
    async moraLengthMoraLengthPostRaw(requestParameters: MoraLengthMoraLengthPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling moraLengthMoraLengthPost.');
        }

        if (requestParameters.accentPhrase === null || requestParameters.accentPhrase === undefined) {
            throw new runtime.RequiredError('accentPhrase','Required parameter requestParameters.accentPhrase was null or undefined when calling moraLengthMoraLengthPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/mora_length`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: requestParameters.accentPhrase.map(AccentPhraseToJSON),
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(AccentPhraseFromJSON));
    }

    /**
     * アクセント句から音素長を得る
     */
    async moraLengthMoraLengthPost(requestParameters: MoraLengthMoraLengthPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>> {
        const response = await this.moraLengthMoraLengthPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * アクセント句から音高を得る
     */
    async moraPitchMoraPitchPostRaw(requestParameters: MoraPitchMoraPitchPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<AccentPhrase>>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling moraPitchMoraPitchPost.');
        }

        if (requestParameters.accentPhrase === null || requestParameters.accentPhrase === undefined) {
            throw new runtime.RequiredError('accentPhrase','Required parameter requestParameters.accentPhrase was null or undefined when calling moraPitchMoraPitchPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/mora_pitch`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: requestParameters.accentPhrase.map(AccentPhraseToJSON),
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(AccentPhraseFromJSON));
    }

    /**
     * アクセント句から音高を得る
     */
    async moraPitchMoraPitchPost(requestParameters: MoraPitchMoraPitchPostRequest, initOverrides?: RequestInit): Promise<Array<AccentPhrase>> {
        const response = await this.moraPitchMoraPitchPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * 複数まとめて音声合成する
     */
    async multiSynthesisMultiSynthesisPostRaw(requestParameters: MultiSynthesisMultiSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling multiSynthesisMultiSynthesisPost.');
        }

        if (requestParameters.audioQuery === null || requestParameters.audioQuery === undefined) {
            throw new runtime.RequiredError('audioQuery','Required parameter requestParameters.audioQuery was null or undefined when calling multiSynthesisMultiSynthesisPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/multi_synthesis`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: requestParameters.audioQuery.map(AudioQueryToJSON),
        }, initOverrides);

        return new runtime.BlobApiResponse(response);
    }

    /**
     * 複数まとめて音声合成する
     */
    async multiSynthesisMultiSynthesisPost(requestParameters: MultiSynthesisMultiSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob> {
        const response = await this.multiSynthesisMultiSynthesisPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * 指定されたspeaker_uuidに関する情報をjson形式で返します。 画像や音声はbase64エンコードされたものが返されます。  Returns ------- ret_data: SpeakerInfo
     * Speaker Info
     */
    async speakerInfoSpeakerInfoGetRaw(requestParameters: SpeakerInfoSpeakerInfoGetRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<SpeakerInfo>> {
        if (requestParameters.speakerUuid === null || requestParameters.speakerUuid === undefined) {
            throw new runtime.RequiredError('speakerUuid','Required parameter requestParameters.speakerUuid was null or undefined when calling speakerInfoSpeakerInfoGet.');
        }

        const queryParameters: any = {};

        if (requestParameters.speakerUuid !== undefined) {
            queryParameters['speaker_uuid'] = requestParameters.speakerUuid;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/speaker_info`,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => SpeakerInfoFromJSON(jsonValue));
    }

    /**
     * 指定されたspeaker_uuidに関する情報をjson形式で返します。 画像や音声はbase64エンコードされたものが返されます。  Returns ------- ret_data: SpeakerInfo
     * Speaker Info
     */
    async speakerInfoSpeakerInfoGet(requestParameters: SpeakerInfoSpeakerInfoGetRequest, initOverrides?: RequestInit): Promise<SpeakerInfo> {
        const response = await this.speakerInfoSpeakerInfoGetRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * Speakers
     */
    async speakersSpeakersGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<Array<Speaker>>> {
        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/speakers`,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => jsonValue.map(SpeakerFromJSON));
    }

    /**
     * Speakers
     */
    async speakersSpeakersGet(initOverrides?: RequestInit): Promise<Array<Speaker>> {
        const response = await this.speakersSpeakersGetRaw(initOverrides);
        return await response.value();
    }

    /**
     * 指定された2人の話者で音声を合成、指定した割合でモーフィングした音声を得ます。 モーフィングの割合は`morph_rate`で指定でき、0.0でベースの話者、1.0でターゲットの話者に近づきます。
     * 2人の話者でモーフィングした音声を合成する
     */
    async synthesisMorphingSynthesisMorphingPostRaw(requestParameters: SynthesisMorphingSynthesisMorphingPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>> {
        if (requestParameters.baseSpeaker === null || requestParameters.baseSpeaker === undefined) {
            throw new runtime.RequiredError('baseSpeaker','Required parameter requestParameters.baseSpeaker was null or undefined when calling synthesisMorphingSynthesisMorphingPost.');
        }

        if (requestParameters.targetSpeaker === null || requestParameters.targetSpeaker === undefined) {
            throw new runtime.RequiredError('targetSpeaker','Required parameter requestParameters.targetSpeaker was null or undefined when calling synthesisMorphingSynthesisMorphingPost.');
        }

        if (requestParameters.morphRate === null || requestParameters.morphRate === undefined) {
            throw new runtime.RequiredError('morphRate','Required parameter requestParameters.morphRate was null or undefined when calling synthesisMorphingSynthesisMorphingPost.');
        }

        if (requestParameters.audioQuery === null || requestParameters.audioQuery === undefined) {
            throw new runtime.RequiredError('audioQuery','Required parameter requestParameters.audioQuery was null or undefined when calling synthesisMorphingSynthesisMorphingPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.baseSpeaker !== undefined) {
            queryParameters['base_speaker'] = requestParameters.baseSpeaker;
        }

        if (requestParameters.targetSpeaker !== undefined) {
            queryParameters['target_speaker'] = requestParameters.targetSpeaker;
        }

        if (requestParameters.morphRate !== undefined) {
            queryParameters['morph_rate'] = requestParameters.morphRate;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/synthesis_morphing`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: AudioQueryToJSON(requestParameters.audioQuery),
        }, initOverrides);

        return new runtime.BlobApiResponse(response);
    }

    /**
     * 指定された2人の話者で音声を合成、指定した割合でモーフィングした音声を得ます。 モーフィングの割合は`morph_rate`で指定でき、0.0でベースの話者、1.0でターゲットの話者に近づきます。
     * 2人の話者でモーフィングした音声を合成する
     */
    async synthesisMorphingSynthesisMorphingPost(requestParameters: SynthesisMorphingSynthesisMorphingPostRequest, initOverrides?: RequestInit): Promise<Blob> {
        const response = await this.synthesisMorphingSynthesisMorphingPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * 音声合成する
     */
    async synthesisSynthesisPostRaw(requestParameters: SynthesisSynthesisPostRequest, initOverrides?: RequestInit): Promise<runtime.ApiResponse<Blob>> {
        if (requestParameters.speaker === null || requestParameters.speaker === undefined) {
            throw new runtime.RequiredError('speaker','Required parameter requestParameters.speaker was null or undefined when calling synthesisSynthesisPost.');
        }

        if (requestParameters.audioQuery === null || requestParameters.audioQuery === undefined) {
            throw new runtime.RequiredError('audioQuery','Required parameter requestParameters.audioQuery was null or undefined when calling synthesisSynthesisPost.');
        }

        const queryParameters: any = {};

        if (requestParameters.speaker !== undefined) {
            queryParameters['speaker'] = requestParameters.speaker;
        }

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/synthesis`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: AudioQueryToJSON(requestParameters.audioQuery),
        }, initOverrides);

        return new runtime.BlobApiResponse(response);
    }

    /**
     * 音声合成する
     */
    async synthesisSynthesisPost(requestParameters: SynthesisSynthesisPostRequest, initOverrides?: RequestInit): Promise<Blob> {
        const response = await this.synthesisSynthesisPostRaw(requestParameters, initOverrides);
        return await response.value();
    }

    /**
     * Version
     */
    async versionVersionGetRaw(initOverrides?: RequestInit): Promise<runtime.ApiResponse<any>> {
        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/version`,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.TextApiResponse(response) as any;
    }

    /**
     * Version
     */
    async versionVersionGet(initOverrides?: RequestInit): Promise<any> {
        const response = await this.versionVersionGetRaw(initOverrides);
        return await response.value();
    }

}
